{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cfda3fc",
   "metadata": {},
   "source": [
    "# AI lab - week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ce392",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "batch_size_train = 16\n",
    "batch_size_test = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "#device = torch.device(\"mps\") # for GPU usage on Apple Silicon\n",
    "\n",
    "columns = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]\n",
    "csv_path = '../data/titanic.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdf22a",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_NaNs(df, drop=False):\n",
    "  if drop:\n",
    "    dfc = df.dropna()\n",
    "    return dfc\n",
    "  dfc = df.copy()\n",
    "  categorical_columns = dfc.select_dtypes(exclude=np.number).columns\n",
    "  imp_freq = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "  dfc.loc[:, categorical_columns] = imp_freq.fit_transform(dfc[categorical_columns])\n",
    "\n",
    "  numeric_columns = dfc.select_dtypes(include=np.number).columns\n",
    "  imp_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "  dfc.loc[:, numeric_columns] = imp_mean.fit_transform(dfc[numeric_columns])\n",
    "  return dfc\n",
    "\n",
    "\n",
    "def scale(X_train, X_test):\n",
    "  scaler = MinMaxScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "  return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "  X = df.drop(columns=[\"Survived\"])\n",
    "  y = df[\"Survived\"]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "  X_train = X_train.reset_index(drop=True)\n",
    "  X_test = X_test.reset_index(drop=True)\n",
    "  y_train = y_train.reset_index(drop=True)\n",
    "  y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "  df = impute_NaNs(df, drop=True)\n",
    "  df = pd.get_dummies(df)\n",
    "  X_train, X_test, y_train, y_test = split_data(df)\n",
    "  X_train_scaled, X_test_scaled, _ = scale(X_train, X_test)\n",
    "\n",
    "  return X_train_scaled, X_test_scaled, y_train, y_test, X_train.columns\n",
    "\n",
    "\n",
    "titanic_df = pd.read_csv(csv_path)[columns]\n",
    "X_train_scaled, _, _, _, _ = prepare_data(titanic_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c4b57",
   "metadata": {},
   "source": [
    "## Dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTitanic(Dataset):\n",
    "  def __init__(self, df_path, train=True):\n",
    "    columns = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]\n",
    "    titanic_df = pd.read_csv(df_path)[columns]\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, new_columns = prepare_data(titanic_df)\n",
    "\n",
    "    X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    self.X = X_train_scaled if train else X_test_scaled\n",
    "    self.y = y_train if train else y_test\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "titanic_train = CustomTitanic(csv_path, train=True)\n",
    "titanic_val = CustomTitanic(csv_path, train=False)\n",
    "\n",
    "train_loader = DataLoader(titanic_train, batch_size=batch_size_train, shuffle=True)\n",
    "val_loader = DataLoader(titanic_val, batch_size=batch_size_test, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956caf5f",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8994d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NN(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out, dropout=0.5):\n",
    "    super(NN, self).__init__()\n",
    "    self.hidden = nn.ModuleList()\n",
    "    self.hidden.append(nn.Linear(D_in, H[0]))\n",
    "    if dropout > 0:\n",
    "      self.hidden.append(nn.Dropout(dropout))\n",
    "    self.hidden.append(nn.ReLU())\n",
    "\n",
    "    for i in range(1, len(H)):\n",
    "      self.hidden.append(nn.Linear(H[i-1], H[i]))\n",
    "      if dropout > 0:\n",
    "        self.hidden.append(nn.Dropout(dropout))\n",
    "      self.hidden.append(nn.ReLU())\n",
    "\n",
    "    self.output = nn.Linear(H[-1], D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.hidden:\n",
    "      x = layer(x)\n",
    "    x = self.output(x)\n",
    "    return F.sigmoid(x).squeeze()\n",
    "\n",
    "\n",
    "in_dim = X_train_scaled.shape[1]\n",
    "out_dim = 1\n",
    "models_to_train = {\n",
    "  \"model_1\": NN(in_dim, [3], out_dim, dropout=0.0).to(device),\n",
    "  \"model_2\": NN(in_dim, [16], out_dim, dropout=0.0).to(device),\n",
    "  \"model_3\": NN(in_dim, [64, 64], out_dim, dropout=0.0).to(device),\n",
    "  \"model_4\": NN(in_dim, [64, 64], out_dim, dropout=0.5).to(device),\n",
    "  \"model_5\": NN(in_dim, [512, 256, 128], out_dim, dropout=0.0).to(device),\n",
    "  \"model_6\": NN(in_dim, [512, 256, 128], out_dim, dropout=0.5).to(device),\n",
    "  \"model_7\": NN(in_dim, [512, 256, 128], out_dim, dropout=0.8).to(device),\n",
    "  \"model_8\": NN(in_dim, [1024, 512, 256, 128], out_dim, dropout=0.0).to(device),\n",
    "  \"model_9\": NN(in_dim, [1024, 512, 256, 128], out_dim, dropout=0.5).to(device),\n",
    "  \"model_10\": NN(in_dim, [1024, 512, 256, 128], out_dim, dropout=0.8).to(device),\n",
    "}\n",
    "\n",
    "loss_fn = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cd4c3",
   "metadata": {},
   "source": [
    "## Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(n_epochs, model, device, train_loader, learning_rate, momentum, loss_fn):\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "  train_data = defaultdict(list)\n",
    "\n",
    "  for ep in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = list()\n",
    "    epoch_acc = list()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = model(data)\n",
    "      loss = loss_fn(y_pred, target)\n",
    "      train_data[\"loss\"].append(loss.item())\n",
    "      epoch_losses.append(loss.item())\n",
    "      acc = ((y_pred > 0.5).float() == target).float().mean().item()\n",
    "      epoch_acc.append(acc)\n",
    "      train_data[\"acc\"].append(acc)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    train_data[\"epoch_loss\"].append(sum(epoch_losses) / len(epoch_losses))\n",
    "    train_data[\"epoch_acc\"].append(sum(epoch_acc) / len(epoch_acc))\n",
    "    test_results = test(model, device, val_loader, loss_fn)\n",
    "    train_data[\"test_loss\"].append(sum(test_results[\"loss\"]) / len(test_results[\"loss\"]))\n",
    "    train_data[\"test_acc\"].append(sum(test_results[\"acc\"]) / len(test_results[\"acc\"]))\n",
    "\n",
    "  return train_data\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "  model.eval()\n",
    "  test_data = defaultdict(list)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      y_pred = model(data)\n",
    "      loss = loss_fn(y_pred, target)\n",
    "      test_data[\"loss\"].append(loss.item())\n",
    "      acc = ((y_pred > 0.5).float() == target).float().mean().item()\n",
    "      test_data[\"acc\"].append(acc)\n",
    "  return test_data\n",
    "\n",
    "\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "\n",
    "for model_name, model in models_to_train.items():\n",
    "  train_data[model_name] = train(n_epochs, model, device, train_loader, learning_rate, momentum, loss_fn)\n",
    "  test_data[model_name] = test(model, device, val_loader, loss_fn)\n",
    "  train_mean_loss = sum(train_data[model_name][\"loss\"]) / len(train_data[model_name][\"loss\"])\n",
    "  train_mean_acc = sum(train_data[model_name][\"acc\"]) / len(train_data[model_name][\"acc\"]) * 100\n",
    "  test_mean_loss = sum(test_data[model_name][\"loss\"]) / len(test_data[model_name][\"loss\"])\n",
    "  test_mean_acc = sum(test_data[model_name][\"acc\"]) / len(test_data[model_name][\"acc\"]) * 100\n",
    "\n",
    "  print(f\"#### {model_name} ####\")\n",
    "  print(f\"Train mean loss: {train_mean_loss:.4f}\")\n",
    "  print(f\"Train mean accuracy: {train_mean_acc:.2f}%\")\n",
    "  print(f\"Test mean loss: {test_mean_loss:.4f}\")\n",
    "  print(f\"Test mean accuracy: {test_mean_acc:.2f}%\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513283d",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_metrics(train_data, test_data, title):\n",
    "  print(title)\n",
    "  print(f\"Training: loss: {train_data['epoch_loss'][-1]:.2f}, acc: {train_data['epoch_acc'][-1]:.2f}\")\n",
    "  print(f\"Testing: loss: {sum(test_data['loss']) / len(test_data['loss']):.2f}, acc: {sum(test_data['acc']) / len(test_data['acc']):.2f}\")\n",
    "\n",
    "\n",
    "def plot_metrics(train_data, title):\n",
    "  fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "  axs[0].plot(train_data[\"epoch_loss\"], label=\"train\")\n",
    "  axs[0].plot(train_data[\"test_loss\"], label=\"test\")\n",
    "  axs[0].set_title(\"Loss\")\n",
    "  axs[0].legend()\n",
    "  axs[1].plot(train_data[\"epoch_acc\"], label=\"train\")\n",
    "  axs[1].plot(train_data[\"test_acc\"], label=\"test\")\n",
    "  axs[1].set_title(\"Accuracy\")\n",
    "  axs[1].legend()\n",
    "\n",
    "  plt.suptitle(title)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "for model_name in models_to_train.keys():\n",
    "  print_metrics(train_data[model_name], test_data[model_name], model_name)\n",
    "  plot_metrics(train_data[model_name], model_name)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
